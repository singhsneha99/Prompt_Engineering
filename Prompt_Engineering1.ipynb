{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPypSGNt8e10IZymKObSTC5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a349cc44bfef4f26bc2e01f55ce5cc8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_645e26babc854bf0bac92c3b08d09bcd",
              "IPY_MODEL_c648180de9db4675bea58dc748891461"
            ],
            "layout": "IPY_MODEL_299dbbfe5c5f49f896cada33789f95cb"
          }
        },
        "645e26babc854bf0bac92c3b08d09bcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26f8bd59c6a44df8b91002f1d6f37103",
            "placeholder": "​",
            "style": "IPY_MODEL_8f57b1b9c43e4c3c9e1ce0ce69b092d6",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "c648180de9db4675bea58dc748891461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f20005b6f7104cce971bc8bcc908bc8e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c38d77e5d174dddb0baebe611514320",
            "value": 1
          }
        },
        "299dbbfe5c5f49f896cada33789f95cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26f8bd59c6a44df8b91002f1d6f37103": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f57b1b9c43e4c3c9e1ce0ce69b092d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f20005b6f7104cce971bc8bcc908bc8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c38d77e5d174dddb0baebe611514320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "febd6deda19540c2811237d0580e367b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7edbeca9fb748d2949c67c580acb943",
              "IPY_MODEL_5df2431ee72b44999af3059beb4372e8"
            ],
            "layout": "IPY_MODEL_3b1dbfc2579a4cfc9fa9c5970b355c70"
          }
        },
        "b7edbeca9fb748d2949c67c580acb943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27d20c5bc45e464fb2b3f3347fb3c338",
            "placeholder": "​",
            "style": "IPY_MODEL_ceb75cabee434a92b50b9d48ba79188b",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "5df2431ee72b44999af3059beb4372e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94ba4b4df33341c78ad80d8e0137882c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb063685b4534a93bcf446ec37d7d1e9",
            "value": 1
          }
        },
        "3b1dbfc2579a4cfc9fa9c5970b355c70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27d20c5bc45e464fb2b3f3347fb3c338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceb75cabee434a92b50b9d48ba79188b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94ba4b4df33341c78ad80d8e0137882c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb063685b4534a93bcf446ec37d7d1e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai prettytable tqdm tenacity wandb -qq"
      ],
      "metadata": {
        "id": "EuI4WHi-BSKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "import time\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import wandb\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "openai.api_key = 'your-api-key' # enter your OpenAI API key here\n",
        "\n",
        "use_wandb = True # set to True if you want to use wandb to log your config and results\n",
        "\n",
        "use_portkey = True #set to True if you want to use Portkey to log all the prompt chains and their responses Check https://portkey.ai/\n"
      ],
      "metadata": {
        "id": "-EIkiA-UD0FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_gen_system_prompt = \"\"\"Your job is to generate system prompts for GPT-4, given a description of the use-case and some test cases.\n",
        "\n",
        "The prompts you will be generating will be for freeform tasks, such as generating a landing page headline, an intro paragraph, solving a math problem, etc.\n",
        "\n",
        "In your generated prompt, you should describe how the AI should behave in plain English. Include what it will see, and what it's allowed to output. Be creative with prompts to get the best possible results. The AI knows it's an AI -- you don't need to tell it this.\n",
        "\n",
        "You will be graded based on the performance of your prompt... but don't cheat! You cannot include specifics about the test cases in your prompt. Any prompts with examples will be disqualified.\n",
        "\n",
        "Most importantly, output NOTHING but the prompt. Do not include anything else in your message.\"\"\"\n",
        "\n",
        "\n",
        "ranking_system_prompt = \"\"\"Your job is to rank the quality of two outputs generated by different prompts. The prompts are used to generate a response for a given task.\n",
        "\n",
        "You will be provided with the task description, the test prompt, and two generations - one for each system prompt.\n",
        "\n",
        "Rank the generations in order of quality. If Generation A is better, respond with 'A'. If Generation B is better, respond with 'B'.\n",
        "\n",
        "Remember, to be considered 'better', a generation must not just be good, it must be noticeably superior to the other.\n",
        "\n",
        "Also, keep in mind that you are a very harsh critic. Only rank a generation as better if it truly impresses you more than the other.\n",
        "\n",
        "Respond with your ranking, and nothing else. Be fair and unbiased in your judgement.\"\"\""
      ],
      "metadata": {
        "id": "uMz1fpbPBruK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K is a constant factor that determines how much ratings change\n",
        "K = 32\n",
        "\n",
        "CANDIDATE_MODEL = 'gpt-4'\n",
        "CANDIDATE_MODEL_TEMPERATURE = 0.9\n",
        "\n",
        "GENERATION_MODEL = 'gpt-3.5-turbo'\n",
        "GENERATION_MODEL_TEMPERATURE = 0.8\n",
        "GENERATION_MODEL_MAX_TOKENS = 60\n",
        "\n",
        "N_RETRIES = 3  # number of times to retry a call to the ranking model if it fails\n",
        "RANKING_MODEL = 'gpt-3.5-turbo'\n",
        "RANKING_MODEL_TEMPERATURE = 0.5\n",
        "\n",
        "NUMBER_OF_PROMPTS = 10 # this determines how many candidate prompts to generate... the higher, the more expensive, but the better the results will be\n",
        "\n",
        "WANDB_PROJECT_NAME = \"gpt-prompt-eng\" # used if use_wandb is True, Weights &| Biases project name\n",
        "WANDB_RUN_NAME = None # used if use_wandb is True, optionally set the Weights & Biases run name to identify this run\n",
        "\n",
        "PORTKEY_API = \"your-portkey\" # used if use_portkey is True. Get api key here: https://app.portkey.ai/ (click on profile photo on top left)\n",
        "PORTKEY_TRACE = \"prompt_engineer_test_run\" # used if use_portkey is True. Trace each run with a separate ID to differentiate prompt chains\n",
        "HEADERS = {} # don't change. headers will auto populate if use_portkey is true.\n"
      ],
      "metadata": {
        "id": "pZ_MA7uCCPlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start_wandb_run():\n",
        "  # start a new wandb run and log the config\n",
        "  wandb.init(\n",
        "    project=WANDB_PROJECT_NAME,\n",
        "    name=WANDB_RUN_NAME,\n",
        "    config={\n",
        "      \"K\": K,\n",
        "      \"system_gen_system_prompt\": system_gen_system_prompt,\n",
        "      \"ranking_system_prompt\": ranking_system_prompt,\n",
        "      \"candiate_model\": CANDIDATE_MODEL,\n",
        "      \"candidate_model_temperature\": CANDIDATE_MODEL_TEMPERATURE,\n",
        "      \"generation_model\": GENERATION_MODEL,\n",
        "      \"generation_model_temperature\": GENERATION_MODEL_TEMPERATURE,\n",
        "      \"generation_model_max_tokens\": GENERATION_MODEL_MAX_TOKENS,\n",
        "      \"n_retries\": N_RETRIES,\n",
        "      \"ranking_model\": RANKING_MODEL,\n",
        "      \"ranking_model_temperature\": RANKING_MODEL_TEMPERATURE,\n",
        "      \"number_of_prompts\": NUMBER_OF_PROMPTS\n",
        "      })\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "c3XDQC4LDi49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional logging to Weights & Biases to reocrd the configs, prompts and results\n",
        "if use_wandb:\n",
        "  start_wandb_run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256,
          "referenced_widgets": [
            "a349cc44bfef4f26bc2e01f55ce5cc8e",
            "645e26babc854bf0bac92c3b08d09bcd",
            "c648180de9db4675bea58dc748891461",
            "299dbbfe5c5f49f896cada33789f95cb",
            "26f8bd59c6a44df8b91002f1d6f37103",
            "8f57b1b9c43e4c3c9e1ce0ce69b092d6",
            "f20005b6f7104cce971bc8bcc908bc8e",
            "2c38d77e5d174dddb0baebe611514320",
            "febd6deda19540c2811237d0580e367b",
            "b7edbeca9fb748d2949c67c580acb943",
            "5df2431ee72b44999af3059beb4372e8",
            "3b1dbfc2579a4cfc9fa9c5970b355c70",
            "27d20c5bc45e464fb2b3f3347fb3c338",
            "ceb75cabee434a92b50b9d48ba79188b",
            "94ba4b4df33341c78ad80d8e0137882c",
            "fb063685b4534a93bcf446ec37d7d1e9"
          ]
        },
        "id": "6fnPGnkVEGnc",
        "outputId": "30fc1992-c67a-45ed-c774-f0dcd7c32e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:ird1fbkm) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a349cc44bfef4f26bc2e01f55ce5cc8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">charmed-jazz-2</strong> at: <a href='https://wandb.ai/singhsneha1/gpt-prompt-eng/runs/ird1fbkm' target=\"_blank\">https://wandb.ai/singhsneha1/gpt-prompt-eng/runs/ird1fbkm</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240112_192456-ird1fbkm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:ird1fbkm). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112992999995994, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "febd6deda19540c2811237d0580e367b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240112_192907-fesaisa2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/singhsneha1/gpt-prompt-eng/runs/fesaisa2' target=\"_blank\">peachy-snow-3</a></strong> to <a href='https://wandb.ai/singhsneha1/gpt-prompt-eng' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/singhsneha1/gpt-prompt-eng' target=\"_blank\">https://wandb.ai/singhsneha1/gpt-prompt-eng</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/singhsneha1/gpt-prompt-eng/runs/fesaisa2' target=\"_blank\">https://wandb.ai/singhsneha1/gpt-prompt-eng/runs/fesaisa2</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def start_portkey_run():\n",
        "  # define Portkey headers to start logging all prompts & their responses\n",
        "  openai.api_base=\"https://api.portkey.ai/v1/proxy\"\n",
        "  HEADERS = {\n",
        "    \"x-portkey-api-key\": PORTKEY_API,\n",
        "    \"x-portkey-mode\": \"proxy openai\",\n",
        "    \"x-portkey-trace-id\": PORTKEY_TRACE,\n",
        "    #\"x-portkey-retry-count\": 5 # perform automatic retries with exponential backoff if the OpenAI requests fails\n",
        "  }\n",
        "  return HEADERS"
      ],
      "metadata": {
        "id": "_VnmYlXLEJHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional prompt & responses logging\n",
        "if use_portkey:\n",
        "    HEADERS=start_portkey_run()"
      ],
      "metadata": {
        "id": "ffho0XXoELy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_candidate_prompts(description, test_cases, number_of_prompts):\n",
        "  outputs = openai.ChatCompletion.create(\n",
        "      model=CANDIDATE_MODEL, # change this to gpt-3.5-turbo if you don't have GPT-4 access\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": system_gen_system_prompt},\n",
        "          {\"role\": \"user\", \"content\": f\"Here are some test cases:`{test_cases}`\\n\\nHere is the description of the use-case: `{description.strip()}`\\n\\nRespond with your prompt, and nothing else. Be creative.\"}\n",
        "          ],\n",
        "      temperature=CANDIDATE_MODEL_TEMPERATURE,\n",
        "      n=number_of_prompts,\n",
        "      headers=HEADERS)\n",
        "\n",
        "  prompts = []\n",
        "\n",
        "  for i in outputs.choices:\n",
        "    prompts.append(i.message.content)\n",
        "  return prompts\n",
        "\n",
        "def expected_score(r1, r2):\n",
        "    return 1 / (1 + 10**((r2 - r1) / 400))\n",
        "\n",
        "def update_elo(r1, r2, score1):\n",
        "    e1 = expected_score(r1, r2)\n",
        "    e2 = expected_score(r2, r1)\n",
        "    return r1 + K * (score1 - e1), r2 + K * ((1 - score1) - e2)\n",
        "\n",
        "# Get Score - retry up to N_RETRIES times, waiting exponentially between retries.\n",
        "@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))\n",
        "def get_score(description, test_case, pos1, pos2, ranking_model_name, ranking_model_temperature):\n",
        "    score = openai.ChatCompletion.create(\n",
        "        model=ranking_model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": ranking_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"Task: {description.strip()}\n",
        "Prompt: {test_case['prompt']}\n",
        "Generation A: {pos1}\n",
        "Generation B: {pos2}\"\"\"}\n",
        "        ],\n",
        "        logit_bias={\n",
        "              '32': 100,  # 'A' token\n",
        "              '33': 100,  # 'B' token\n",
        "        },\n",
        "        max_tokens=1,\n",
        "        temperature=ranking_model_temperature,\n",
        "        headers=HEADERS,\n",
        "    ).choices[0].message.content\n",
        "    return score\n",
        "\n",
        "@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))\n",
        "def get_generation(prompt, test_case):\n",
        "    generation = openai.ChatCompletion.create(\n",
        "        model=GENERATION_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"{test_case['prompt']}\"}\n",
        "        ],\n",
        "        max_tokens=GENERATION_MODEL_MAX_TOKENS,\n",
        "        temperature=GENERATION_MODEL_TEMPERATURE,\n",
        "        headers=HEADERS,\n",
        "    ).choices[0].message.content\n",
        "    return generation\n",
        "\n",
        "def test_candidate_prompts(test_cases, description, prompts):\n",
        "  # Initialize each prompt with an ELO rating of 1200\n",
        "  prompt_ratings = {prompt: 1200 for prompt in prompts}\n",
        "\n",
        "  # Calculate total rounds for progress bar\n",
        "  total_rounds = len(test_cases) * len(prompts) * (len(prompts) - 1) // 2\n",
        "\n",
        "  # Initialize progress bar\n",
        "  pbar = tqdm(total=total_rounds, ncols=70)\n",
        "\n",
        "  # For each pair of prompts\n",
        "  for prompt1, prompt2 in itertools.combinations(prompts, 2):\n",
        "      # For each test case\n",
        "      for test_case in test_cases:\n",
        "          # Update progress bar\n",
        "          pbar.update()\n",
        "\n",
        "          # Generate outputs for each prompt\n",
        "          generation1 = get_generation(prompt1, test_case)\n",
        "          generation2 = get_generation(prompt2, test_case)\n",
        "\n",
        "          # Rank the outputs\n",
        "          score1 = get_score(description, test_case, generation1, generation2, RANKING_MODEL, RANKING_MODEL_TEMPERATURE)\n",
        "          score2 = get_score(description, test_case, generation2, generation1, RANKING_MODEL, RANKING_MODEL_TEMPERATURE)\n",
        "\n",
        "          # Convert scores to numeric values\n",
        "          score1 = 1 if score1 == 'A' else 0 if score1 == 'B' else 0.5\n",
        "          score2 = 1 if score2 == 'B' else 0 if score2 == 'A' else 0.5\n",
        "\n",
        "          # Average the scores\n",
        "          score = (score1 + score2) / 2\n",
        "\n",
        "          # Update ELO ratings\n",
        "          r1, r2 = prompt_ratings[prompt1], prompt_ratings[prompt2]\n",
        "          r1, r2 = update_elo(r1, r2, score)\n",
        "          prompt_ratings[prompt1], prompt_ratings[prompt2] = r1, r2\n",
        "\n",
        "          # Print the winner of this round\n",
        "          if score > 0.5:\n",
        "              print(f\"Winner: {prompt1}\")\n",
        "          elif score < 0.5:\n",
        "              print(f\"Winner: {prompt2}\")\n",
        "          else:\n",
        "              print(\"Draw\")\n",
        "\n",
        "  # Close progress bar\n",
        "  pbar.close()\n",
        "\n",
        "  return prompt_ratings\n",
        "\n",
        "def generate_optimal_prompt(description, test_cases, number_of_prompts=10, use_wandb=False):\n",
        "  if use_wandb:\n",
        "    wandb_table = wandb.Table(columns=[\"Prompt\", \"Ranking\"])\n",
        "    if wandb.run is None:\n",
        "      start_wandb_run()\n",
        "\n",
        "  prompts = generate_candidate_prompts(description, test_cases, number_of_prompts)\n",
        "  prompt_ratings = test_candidate_prompts(test_cases, description, prompts)\n",
        "\n",
        "  # Print the final ELO ratingsz\n",
        "  table = PrettyTable()\n",
        "  table.field_names = [\"Prompt\", \"Rating\"]\n",
        "  for prompt, rating in sorted(prompt_ratings.items(), key=lambda item: item[1], reverse=True):\n",
        "      table.add_row([prompt, rating])\n",
        "      if use_wandb:\n",
        "         wandb_table.add_data(prompt, rating)\n",
        "\n",
        "  if use_wandb: # log the results to a Weights & Biases table and finsih the run\n",
        "    wandb.log({\"prompt_ratings\": wandb_table})\n",
        "    wandb.finish()\n",
        "  print(table)\n"
      ],
      "metadata": {
        "id": "4j2a0N3lEN77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inserting Specific Test Cases:"
      ],
      "metadata": {
        "id": "1eM_EmUiE0WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "description = \"Given a prompt, generate a landing page headline.\" # this style of description tends to work well\n",
        "\n",
        "test_cases = [\n",
        "  {\n",
        "    \"prompt\": \"Explain the concept of short selling a stock in simple terms\"\n",
        "  },\n",
        "  {\n",
        "    \"prompt\": \"Provide a 200 word overview of quantitative easing monetary policy\"\n",
        "  },\n",
        "  {\n",
        "    \"prompt\": \"Compare the Bloomberg terminals to alternative financial data platforms\"\n",
        "  },\n",
        "  {\n",
        "    \"prompt\": \"Describe the typical workflow of a hedge fund analyst\"\n",
        "  },\n",
        "  {\n",
        "    \"prompt\": \"Explain the capital asset pricing model formula in simple terms\"\n",
        "  },\n",
        "  {\n",
        "    \"prompt\": \"Provide 3 creative examples comparing stocks to different food items\"\n",
        "  },\n",
        "  {\n",
        "    \"prompt\": \"Write a children's story explaining bonds and yields\"\n",
        "  },\n",
        "  {\n",
        "    \"prompt\": \"Analyze the impact of raising interest rates on economic growth\"\n",
        "  },\n",
        "  {\n",
        "    \"prompt\": \"Discuss the role creativity plays in designing fintech solutions\"\n",
        "  },\n",
        "  {\n",
        "    \"prompt\": \"Propose 5 potential startup ideas in decentralized finance\"\n",
        "  }\n",
        "\n",
        "]\n",
        "\n",
        "if use_wandb:\n",
        "    wandb.config.update({\"description\": description,\n",
        "                        \"test_cases\": test_cases})"
      ],
      "metadata": {
        "id": "d7FsUdywE4x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openai migrate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBn68gfhGAZW",
        "outputId": "dbe38f68-8c14-49e3-8458-f7f00c52315f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: openai [-h] [-V] [-v] [-b API_BASE] [-k API_KEY] [-p PROXY [PROXY ...]] [-o ORGANIZATION]\n",
            "              {api,tools,wandb} ...\n",
            "openai: error: argument {api,tools,wandb}: invalid choice: 'migrate' (choose from 'api', 'tools', 'wandb')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_optimal_prompt(description, test_cases, NUMBER_OF_PROMPTS, use_wandb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "3Z2DUG8KFDvt",
        "outputId": "02be5688-3243-4e61-df51-1860c5592c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "APIRemovedInV1",
          "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-5fb2904844c6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_optimal_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUMBER_OF_PROMPTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_wandb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-73-c132a7be29e8>\u001b[0m in \u001b[0;36mgenerate_optimal_prompt\u001b[0;34m(description, test_cases, number_of_prompts, use_wandb)\u001b[0m\n\u001b[1;32m    116\u001b[0m       \u001b[0mstart_wandb_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m   \u001b[0mprompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_candidate_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_prompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m   \u001b[0mprompt_ratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_candidate_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_cases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-c132a7be29e8>\u001b[0m in \u001b[0;36mgenerate_candidate_prompts\u001b[0;34m(description, test_cases, number_of_prompts)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_candidate_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_prompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   outputs = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCANDIDATE_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# change this to gpt-3.5-turbo if you don't have GPT-4 access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       messages=[\n\u001b[1;32m      5\u001b[0m           \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msystem_gen_system_prompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def generate_optimal_prompt(description, test_cases, number_of_prompts, use_wandb):\n",
        "    # Your OpenAI API key\n",
        "    openai.api_key = \"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\"\n",
        "\n",
        "    # Define a prompt based on your use case\n",
        "    prompt = f\"Description: {description}\\nTest Cases:\\n{test_cases}\\nOptimal Prompt:\"\n",
        "\n",
        "    # Generate prompts using OpenAI's GPT-3\n",
        "    prompts = []\n",
        "    for _ in range(number_of_prompts):\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"text-davinci-002\",  # Choose the appropriate engine\n",
        "            prompt=prompt,\n",
        "            max_tokens=150,\n",
        "            temperature=0.7,\n",
        "            n = 1\n",
        "        )\n",
        "        prompts.append(response.choices[0].text.strip())\n",
        "\n",
        "    # Additional processing or logging, if needed\n",
        "    if use_wandb:\n",
        "        import wandb\n",
        "        wandb.log({\"prompts\": prompts})\n",
        "\n",
        "    return prompts\n",
        "\n",
        "# Example usage\n",
        "description = \"Your project description here.\"\n",
        "test_cases = \"Your test cases here.\"\n",
        "NUMBER_OF_PROMPTS = 5\n",
        "use_wandb = True  # Set to True if you want to log prompts using WandB\n",
        "\n",
        "generated_prompts = generate_optimal_prompt(description, test_cases, NUMBER_OF_PROMPTS, use_wandb)\n",
        "print(generated_prompts)\n"
      ],
      "metadata": {
        "id": "m1WwhBflJET3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}